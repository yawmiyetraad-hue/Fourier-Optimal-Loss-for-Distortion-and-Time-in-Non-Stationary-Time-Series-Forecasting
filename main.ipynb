{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpbSFC9Jgd-j",
        "outputId": "4f1ffa66-527b-418e-829c-0a19d537907b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DILATE'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 69 (delta 6), reused 2 (delta 0), pack-reused 52 (from 1)\u001b[K\n",
            "Receiving objects: 100% (69/69), 4.71 MiB | 25.49 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "/content/DILATE\n",
            "Collecting tslearn\n",
            "  Downloading tslearn-0.7.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.4 in /usr/local/lib/python3.12/dist-packages (from tslearn) (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tslearn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from tslearn) (1.16.3)\n",
            "Requirement already satisfied: numba>=0.58.1 in /usr/local/lib/python3.12/dist-packages (from tslearn) (0.60.0)\n",
            "Requirement already satisfied: joblib>=1.2 in /usr/local/lib/python3.12/dist-packages (from tslearn) (1.5.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.58.1->tslearn) (0.43.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4->tslearn) (3.6.0)\n",
            "Downloading tslearn-0.7.0-py3-none-any.whl (372 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.7/372.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tslearn\n",
            "Successfully installed tslearn-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vincent-leguen/DILATE\n",
        "%cd /content/DILATE\n",
        "!pip install tslearn\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "class SDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X_input, X_target):\n",
        "        super(SDataset, self).__init__()\n",
        "        self.X_input = X_input\n",
        "        self.X_target = X_target\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.X_input).shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.X_input[idx,:,np.newaxis], self.X_target[idx,:,np.newaxis] )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EncoderRNN(torch.nn.Module):\n",
        "    def __init__(self,input_size, hidden_size, num_grulstm_layers, batch_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_grulstm_layers = num_grulstm_layers\n",
        "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_grulstm_layers,batch_first=True)\n",
        "\n",
        "    def forward(self, input, hidden): # input [batch_size, length T, dimensionality d]\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self,device):\n",
        "        #[num_layers*num_directions,batch,hidden_size]\n",
        "        return torch.zeros(self.num_grulstm_layers, self.batch_size, self.hidden_size, device=device)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_grulstm_layers,fc_units, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_grulstm_layers,batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, fc_units)\n",
        "        self.out = nn.Linear(fc_units, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "        output = F.relu( self.fc(output) )\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "class Net_GRU(nn.Module):\n",
        "    def __init__(self, encoder, decoder, target_length, device):\n",
        "        super(Net_GRU, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_length = target_length\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_length  = x.shape[1]\n",
        "        encoder_hidden = self.encoder.init_hidden(self.device)\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = self.encoder(x[:,ei:ei+1,:]  , encoder_hidden)\n",
        "\n",
        "        decoder_input = x[:,-1,:].unsqueeze(1) # first decoder input= last element of input sequence\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        outputs = torch.zeros([x.shape[0], self.target_length, x.shape[2]]  ).to(self.device)\n",
        "        for di in range(self.target_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            decoder_input = decoder_output\n",
        "            outputs[:,di:di+1,:] = decoder_output\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "l_yfHAqmgr6A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Charger le fichier CSV\n",
        "file_path = '/content/ETTh1.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Extraire la colonne 'Close'\n",
        "if 'LUFL' in data.columns:\n",
        "    close_prices = data['LUFL'].dropna().values  # Suppression des valeurs NaN si elles existent\n",
        "else:\n",
        "    raise ValueError(\"La colonne 'max. wv (m/s)' n'existe pas dans le fichier.\")\n",
        "\n",
        "# Normalisation des données (Min-Max Scaling entre 0 et 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "close_prices_normalized = scaler.fit_transform(close_prices.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Fonction pour créer les séquences d'entrée et de cible\n",
        "def create_sequences(data, input_size, target_size, stride=1):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(0, len(data) - input_size - target_size + 1, stride):\n",
        "        inputs.append(data[i:i + input_size])\n",
        "        targets.append(data[i + input_size:i + input_size + target_size])\n",
        "    return np.array(inputs), np.array(targets)\n",
        "\n",
        "# Diviser en séquences (25 pour input, 10 pour target)\n",
        "input_size = 25\n",
        "target_size = 10\n",
        "stride = 5\n",
        "\n",
        "X, y = create_sequences(close_prices_normalized, input_size, target_size, stride)\n",
        "\n",
        "print(f\"Dimensions des données d'entrée (X) : {X.shape}\")\n",
        "print(f\"Dimensions des données de sortie (y) : {y.shape}\")\n",
        "\n",
        "# Exemple des premières séquences\n",
        "print(\"Première séquence d'entrée :\", X[0])\n",
        "print(\"Première séquence de sortie :\", y[0])\n",
        "\n",
        "# Pour inverser la normalisation plus tard, vous pouvez utiliser :\n",
        "# scaler.inverse_transform(normalized_data.reshape(-1, 1))\n",
        "X = np.stack(X)\n",
        "y = np.stack(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOYcUFJcg66T",
        "outputId": "567a15d7-e760-4388-9a9a-9a24fba27d1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions des données d'entrée (X) : (3478, 25)\n",
            "Dimensions des données de sortie (y) : (3478, 10)\n",
            "Première séquence d'entrée : [0.5565765  0.55027876 0.51259548 0.51569273 0.5219905  0.54088375\n",
            " 0.64154449 0.64784222 0.42773073 0.39933925 0.42773073 0.47800948\n",
            " 0.41513523 0.4434235  0.38684699 0.38994424 0.39304149 0.44022299\n",
            " 0.44022299 0.43392526 0.41823248 0.39933925 0.39304149 0.37734874\n",
            " 0.37425149]\n",
            "Première séquence de sortie : [0.36475325 0.35845549 0.3490605  0.3490605  0.37425149 0.43082798\n",
            " 0.44022299 0.42453024 0.39933925 0.43712574]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "batch_size = 100\n",
        "N = 200\n",
        "N_input = 25\n",
        "N_output = 10\n",
        "sigma = 0.01\n",
        "gamma = 0.01\n",
        "\n",
        "# Load synthetic dataset\n",
        "X_train_input=X[0:N,0:N_input]\n",
        "X_train_target=y[0:N,0:N_output]\n",
        "X_test_input=X[N:N+200,0:N_input]\n",
        "X_test_target=y[N:N+200, 0:N_output]\n",
        "\n",
        "dataset_train = SDataset(X_train_input,X_train_target)\n",
        "dataset_test  = SDataset(X_test_input,X_test_target)\n",
        "trainloader = DataLoader(dataset_train, batch_size=batch_size,shuffle=True, num_workers=1)\n",
        "testloader  = DataLoader(dataset_test, batch_size=batch_size,shuffle=False, num_workers=1)\n"
      ],
      "metadata": {
        "id": "8Vl9P7iKhYya"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "from loss.dilate_loss import dilate_loss\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from tslearn.metrics import dtw, dtw_path\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "\n",
        "def train_model(net,loss_type, learning_rate, epochs=1000, gamma = 0.001,\n",
        "                print_every=50,eval_every=50, verbose=1, Lambda=1, alpha=0.5):\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, target= data\n",
        "            inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
        "            target = torch.tensor(target, dtype=torch.float32).to(device)\n",
        "            batch_size = target.shape[0]\n",
        "            N_output = 1\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss_mse,loss_shape,loss_temporal = torch.tensor(0),torch.tensor(0),torch.tensor(0)\n",
        "\n",
        "            if (loss_type=='mse'):\n",
        "                loss_mse = criterion(target,outputs)\n",
        "                loss = loss_mse\n",
        "\n",
        "            if (loss_type=='dilate'):\n",
        "                loss, loss_shape, loss_temporal = dilate_loss(target,outputs,alpha, gamma, device)\n",
        "\n",
        "            if (loss_type=='foldt'):\n",
        "                loss, loss_shape, loss_temporal = foldt_loss(target,outputs,alpha, gamma, device)\n",
        "\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if(verbose):\n",
        "            if (epoch % print_every == 0):\n",
        "                print('epoch ', epoch, ' loss ',loss.item(),' loss shape ',loss_shape.item(),' loss temporal ',loss_temporal.item())\n",
        "                eval_model(net,testloader, gamma,verbose=1)\n",
        "\n",
        "\n",
        "def eval_model(net,loader, gamma,verbose=1):\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    losses_mse = []\n",
        "    losses_dtw = []\n",
        "    losses_tdi = []\n",
        "\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        loss_mse, loss_dtw, loss_tdi = torch.tensor(0),torch.tensor(0),torch.tensor(0)\n",
        "        # get the inputs\n",
        "        inputs, target= data\n",
        "        inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
        "        target = torch.tensor(target, dtype=torch.float32).to(device)\n",
        "        batch_size, N_output = target.shape[0:2]\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # MSE\n",
        "        loss_mse = criterion(target,outputs)\n",
        "        loss_dtw, loss_tdi = 0,0\n",
        "        # DTW and TDI\n",
        "        for k in range(batch_size):\n",
        "            target_k_cpu = target[k,:,0:1].view(-1).detach().cpu().numpy()\n",
        "            output_k_cpu = outputs[k,:,0:1].view(-1).detach().cpu().numpy()\n",
        "\n",
        "            path, sim = dtw_path(target_k_cpu, output_k_cpu)\n",
        "            loss_dtw += sim\n",
        "\n",
        "            Dist = 0\n",
        "            for i,j in path:\n",
        "                    Dist += (i-j)*(i-j)\n",
        "            loss_tdi += Dist / (N_output*N_output)\n",
        "\n",
        "        loss_dtw = loss_dtw /batch_size\n",
        "        loss_tdi = loss_tdi / batch_size\n",
        "\n",
        "        # print statistics\n",
        "        losses_mse.append( loss_mse.item() )\n",
        "        losses_dtw.append( loss_dtw )\n",
        "        losses_tdi.append( loss_tdi )\n",
        "\n",
        "    print( ' Eval mse= ', np.array(losses_mse).mean() ,' dtw= ',np.array(losses_dtw).mean() ,' tdi= ', np.array(losses_tdi).mean())"
      ],
      "metadata": {
        "id": "8WLXalsLhB_J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGpSRNZ6hA_y",
        "outputId": "4e748c3d-40dc-4921-c78f-e1ea3a738c5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pot\n",
            "  Downloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.12/dist-packages (from pot) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6 in /usr/local/lib/python3.12/dist-packages (from pot) (1.16.3)\n",
            "Downloading pot-0.9.6.post1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pot\n",
            "Successfully installed pot-0.9.6.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "from numba import jit\n",
        "import ot\n",
        "\n",
        "@jit(nopython=True)\n",
        "def my_max(x, gamma):\n",
        "    max_x = np.max(x)\n",
        "    exp_x = np.exp((x - max_x) / gamma)\n",
        "    Z = np.sum(exp_x)\n",
        "    return gamma * np.log(Z) + max_x, exp_x / Z\n",
        "\n",
        "@jit(nopython=True)\n",
        "def my_min(x, gamma):\n",
        "    min_x, argmax_x = my_max(-x, gamma)\n",
        "    return -min_x, argmax_x\n",
        "\n",
        "@jit(nopython=True)\n",
        "def my_max_hessian_product(p, z, gamma):\n",
        "    return (p * z - p * np.sum(p * z)) / gamma\n",
        "\n",
        "@jit(nopython=True)\n",
        "def my_min_hessian_product(p, z, gamma):\n",
        "    return -my_max_hessian_product(p, z, gamma)\n",
        "\n",
        "@jit(nopython=True)\n",
        "def dtw_grad(theta, gamma):\n",
        "    m = theta.shape[0]\n",
        "    n = theta.shape[1]\n",
        "    V = np.zeros((m + 1, n + 1))\n",
        "    V[:, 0] = 1e10\n",
        "    V[0, :] = 1e10\n",
        "    V[0, 0] = 0\n",
        "\n",
        "    Q = np.zeros((m + 2, n + 2, 3))\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            v, Q[i, j] = my_min(np.array([V[i, j - 1], V[i - 1, j - 1], V[i - 1, j]]), gamma)\n",
        "            V[i, j] = theta[i - 1, j - 1] + v\n",
        "\n",
        "    E = np.zeros((m + 2, n + 2))\n",
        "    E[m + 1, :] = 0\n",
        "    E[:, n + 1] = 0\n",
        "    E[m + 1, n + 1] = 1\n",
        "    Q[m + 1, n + 1] = 1\n",
        "\n",
        "    for i in range(m, 0, -1):\n",
        "        for j in range(n, 0, -1):\n",
        "            E[i, j] = Q[i, j + 1, 0] * E[i, j + 1] + \\\n",
        "                      Q[i + 1, j + 1, 1] * E[i + 1, j + 1] + \\\n",
        "                      Q[i + 1, j, 2] * E[i + 1, j]\n",
        "\n",
        "    return V[m, n], E[1:m + 1, 1:n + 1], Q, E\n",
        "\n",
        "@jit(nopython=True)\n",
        "def dtw_hessian_prod(theta, Z, Q, E, gamma):\n",
        "    m = Z.shape[0]\n",
        "    n = Z.shape[1]\n",
        "\n",
        "    V_dot = np.zeros((m + 1, n + 1))\n",
        "    V_dot[0, 0] = 0\n",
        "\n",
        "    Q_dot = np.zeros((m + 2, n + 2, 3))\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            V_dot[i, j] = Z[i - 1, j - 1] + \\\n",
        "                          Q[i, j, 0] * V_dot[i, j - 1] + \\\n",
        "                          Q[i, j, 1] * V_dot[i - 1, j - 1] + \\\n",
        "                          Q[i, j, 2] * V_dot[i - 1, j]\n",
        "\n",
        "            v = np.array([V_dot[i, j - 1], V_dot[i - 1, j - 1], V_dot[i - 1, j]])\n",
        "            Q_dot[i, j] = my_min_hessian_product(Q[i, j], v, gamma)\n",
        "    E_dot = np.zeros((m + 2, n + 2))\n",
        "\n",
        "    for j in range(n, 0, -1):\n",
        "        for i in range(m, 0, -1):\n",
        "            E_dot[i, j] = Q_dot[i, j + 1, 0] * E[i, j + 1] + \\\n",
        "                          Q[i, j + 1, 0] * E_dot[i, j + 1] + \\\n",
        "                          Q_dot[i + 1, j + 1, 1] * E[i + 1, j + 1] + \\\n",
        "                          Q[i + 1, j + 1, 1] * E_dot[i + 1, j + 1] + \\\n",
        "                          Q_dot[i + 1, j, 2] * E[i + 1, j] + \\\n",
        "                          Q[i + 1, j, 2] * E_dot[i + 1, j]\n",
        "\n",
        "    return V_dot[m, n], E_dot[1:m + 1, 1:n + 1]\n",
        "\n",
        "def compute_ot(x, y, reg=1.0):\n",
        "    cost_matrix = ot.dist(x, y)\n",
        "    ot_matrix = ot.sinkhorn(np.ones(x.shape[0]) / x.shape[0], np.ones(y.shape[0]) / y.shape[0], cost_matrix, reg)\n",
        "    ot_distance = np.sum(ot_matrix * cost_matrix)\n",
        "    return ot_distance\n",
        "\n",
        "class PathDTWBatch(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, D, gamma, use_ot=False, x=None, y=None, reg=1.0): # D.shape: [batch_size, N, N]\n",
        "        batch_size, N, N = D.shape\n",
        "        device = D.device\n",
        "        D_cpu = D.detach().cpu().numpy()\n",
        "        gamma_gpu = torch.FloatTensor([gamma]).to(device)\n",
        "\n",
        "        grad_gpu = torch.zeros((batch_size, N, N)).to(device)\n",
        "        Q_gpu = torch.zeros((batch_size, N + 2, N + 2, 3)).to(device)\n",
        "        E_gpu = torch.zeros((batch_size, N + 2, N + 2)).to(device)\n",
        "\n",
        "        for k in range(batch_size):\n",
        "            _, grad_cpu_k, Q_cpu_k, E_cpu_k = dtw_grad(D_cpu[k], gamma)\n",
        "            grad_gpu[k] = torch.FloatTensor(grad_cpu_k).to(device)\n",
        "            Q_gpu[k] = torch.FloatTensor(Q_cpu_k).to(device)\n",
        "            E_gpu[k] = torch.FloatTensor(E_cpu_k).to(device)\n",
        "\n",
        "        total_loss = torch.mean(grad_gpu, dim=0)\n",
        "\n",
        "        if use_ot and x is not None and y is not None:\n",
        "            ot_distances = [compute_ot(x[k].cpu().numpy(), y[k].cpu().numpy(), reg) for k in range(batch_size)]\n",
        "            ot_loss = torch.tensor(ot_distances, device=dev).mean()\n",
        "            total_loss += ot_loss\n",
        "\n",
        "        ctx.save_for_backward(grad_gpu, D, Q_gpu, E_gpu, gamma_gpu, torch.tensor([use_ot], device=device), x, y, torch.tensor([reg], device=device))\n",
        "        return total_loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        device = grad_output.device\n",
        "        grad_gpu, D_gpu, Q_gpu, E_gpu, gamma, use_ot, x, y, reg = ctx.saved_tensors\n",
        "        D_cpu = D_gpu.detach().cpu().numpy()\n",
        "        Q_cpu = Q_gpu.detach().cpu().numpy()\n",
        "        E_cpu = E_gpu.detach().cpu().numpy()\n",
        "        gamma = gamma.detach().cpu().numpy()[0]\n",
        "        Z = grad_output.detach().cpu().numpy()\n",
        "\n",
        "        batch_size, N, N = D_cpu.shape\n",
        "        Hessian = torch.zeros((batch_size, N, N)).to(device)\n",
        "        for k in range(batch_size):\n",
        "            _, hess_k = dtw_hessian_prod(D_cpu[k], Z, Q_cpu[k], E_cpu[k], gamma)\n",
        "            Hessian[k] = torch.FloatTensor(hess_k).to(device)\n",
        "\n",
        "        # The OT gradient part is not trivial and not implemented here for simplicity\n",
        "        return Hessian, None, None, None, None, None, None, None\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from numba import jit\n",
        "from torch.autograd import Function\n",
        "from torch.fft import fft\n",
        "\n",
        "# Fonction pour calculer les coefficients de Fourier\n",
        "def fourier_coefficients(x, n_coefficients):\n",
        "    # Calculate Fourier coefficients\n",
        "    X_fft = fft(x, dim=1)\n",
        "    return X_fft[:, :n_coefficients]\n",
        "\n",
        "# Fonction pour calculer les distances combinées (DTW + Fourier)\n",
        "def pairwise_distances_with_fourier(x, y=None, n_coefficients=5):\n",
        "    '''\n",
        "    Input: x is a Nxd matrix\n",
        "           y is an optional Mxd matrix\n",
        "           n_coefficients is the number of Fourier coefficients to use\n",
        "    Output: dist is a NxM matrix where dist[i,j] is the combined distance\n",
        "    '''\n",
        "    # Calculate DTW distances\n",
        "    dtw_distances = pairwise_distances(x, y)\n",
        "\n",
        "    # Calculate Fourier coefficients\n",
        "    x_fourier = fourier_coefficients(x, n_coefficients).abs().float()\n",
        "    if y is not None:\n",
        "        y_fourier = fourier_coefficients(y, n_coefficients).abs().float()\n",
        "    else:\n",
        "        y_fourier = x_fourier\n",
        "\n",
        "    # Calculate Fourier distances\n",
        "    x_fourier_norm = (x_fourier**2).sum(1).view(-1, 1)\n",
        "    y_fourier_t = torch.transpose(y_fourier, 0, 1)\n",
        "    y_fourier_norm = (y_fourier**2).sum(1).view(1, -1)\n",
        "\n",
        "    fourier_distances = x_fourier_norm + y_fourier_norm - 2.0 * torch.mm(x_fourier, y_fourier_t)\n",
        "    fourier_distances = torch.clamp(fourier_distances, 0.0, float('inf'))\n",
        "\n",
        "    # Combine distances\n",
        "    combined_distances = dtw_distances + fourier_distances\n",
        "    return combined_distances\n",
        "\n",
        "# Fonction pour calculer les distances pairwise (DTW)\n",
        "def pairwise_distances(x, y=None):\n",
        "    '''\n",
        "    Input: x is a Nxd matrix\n",
        "           y is an optional Mxd matrix\n",
        "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
        "            if y is not given then use 'y=x'.\n",
        "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
        "    '''\n",
        "    x_norm = (x**2).sum(1).view(-1, 1)\n",
        "    if y is not None:\n",
        "        y_t = torch.transpose(y, 0, 1)\n",
        "        y_norm = (y**2).sum(1).view(1, -1)\n",
        "    else:\n",
        "        y_t = torch.transpose(x, 0, 1)\n",
        "        y_norm = x_norm.view(1, -1)\n",
        "\n",
        "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
        "    return torch.clamp(dist, 0.0, float('inf'))\n",
        "\n",
        "@jit(nopython=True)\n",
        "def compute_softdtw(D, gamma):\n",
        "    N = D.shape[0]\n",
        "    M = D.shape[1]\n",
        "    R = np.zeros((N + 2, M + 2)) + 1e8\n",
        "    R[0, 0] = 0\n",
        "    for j in range(1, M + 1):\n",
        "        for i in range(1, N + 1):\n",
        "            r0 = -R[i - 1, j - 1] / gamma\n",
        "            r1 = -R[i - 1, j] / gamma\n",
        "            r2 = -R[i, j - 1] / gamma\n",
        "            rmax = max(max(r0, r1), r2)\n",
        "            rsum = np.exp(r0 - rmax) + np.exp(r1 - rmax) + np.exp(r2 - rmax)\n",
        "            softmin = - gamma * (np.log(rsum) + rmax)\n",
        "            R[i, j] = D[i - 1, j - 1] + softmin\n",
        "    return R\n",
        "\n",
        "@jit(nopython=True)\n",
        "def compute_softdtw_backward(D_, R, gamma):\n",
        "    N = D_.shape[0]\n",
        "    M = D_.shape[1]\n",
        "    D = np.zeros((N + 2, M + 2))\n",
        "    E = np.zeros((N + 2, M + 2))\n",
        "    D[1:N + 1, 1:M + 1] = D_\n",
        "    E[-1, -1] = 1\n",
        "    R[:, -1] = -1e8\n",
        "    R[-1, :] = -1e8\n",
        "    R[-1, -1] = R[-2, -2]\n",
        "    for j in range(M, 0, -1):\n",
        "        for i in range(N, 0, -1):\n",
        "            a0 = (R[i + 1, j] - R[i, j] - D[i + 1, j]) / gamma\n",
        "            b0 = (R[i, j + 1] - R[i, j] - D[i, j + 1]) / gamma\n",
        "            c0 = (R[i + 1, j + 1] - R[i, j] - D[i + 1, j + 1]) / gamma\n",
        "            a = np.exp(a0)\n",
        "            b = np.exp(b0)\n",
        "            c = np.exp(c0)\n",
        "            E[i, j] = E[i + 1, j] * a + E[i, j + 1] * b + E[i + 1, j + 1] * c\n",
        "    return E[1:N + 1, 1:M + 1]\n",
        "\n",
        "class SoftDTWBatch(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, D, gamma=1.0): # D.shape: [batch_size, N , N]\n",
        "        dev = D.device\n",
        "        batch_size, N, N = D.shape\n",
        "        gamma = torch.FloatTensor([gamma]).to(dev)\n",
        "        D_ = D.detach().cpu().numpy()\n",
        "        g_ = gamma.item()\n",
        "\n",
        "        total_loss = 0\n",
        "        R = torch.zeros((batch_size, N+2, N+2)).to(dev)\n",
        "        for k in range(0, batch_size):  # loop over all D in the batch\n",
        "            Rk = torch.FloatTensor(compute_softdtw(D_[k, :, :], g_)).to(dev)\n",
        "            R[k:k+1, :, :] = Rk\n",
        "            total_loss = total_loss + Rk[-2, -2]\n",
        "        ctx.save_for_backward(D, R, gamma)\n",
        "        return total_loss / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        dev = grad_output.device\n",
        "        D, R, gamma = ctx.saved_tensors\n",
        "        batch_size, N, N = D.shape\n",
        "        D_ = D.detach().cpu().numpy()\n",
        "        R_ = R.detach().cpu().numpy()\n",
        "        g_ = gamma.item()\n",
        "\n",
        "        E = torch.zeros((batch_size, N, N)).to(dev)\n",
        "        for k in range(batch_size):\n",
        "            Ek = torch.FloatTensor(compute_softdtw_backward(D_[k, :, :], R_[k, :, :], g_)).to(dev)\n",
        "            E[k:k+1, :, :] = Ek\n",
        "\n",
        "        return grad_output * E, None\n",
        "\n",
        "def combined_dtw_fourier_loss(x, y, gamma=1.0, n_coefficients=3):\n",
        "    dist_matrix = pairwise_distances_with_fourier(x, y, n_coefficients)\n",
        "    loss = SoftDTWBatch.apply(dist_matrix, gamma)\n",
        "    return loss\n",
        "\n",
        "def foldt_loss(outputs, targets, alpha, gamma, device, n_coefficients=6):\n",
        "    # outputs, targets: shape (batch_size, N_output, 1)\n",
        "    batch_size, N_output = outputs.shape[0:2]\n",
        "    loss_shape = 0\n",
        "    softdtw_batch = SoftDTWBatch.apply\n",
        "    D = torch.zeros((batch_size, N_output, N_output)).to(device)\n",
        "\n",
        "    for k in range(batch_size):\n",
        "        # Calculate the combined distances (DTW + Fourier)\n",
        "        Dk = pairwise_distances_with_fourier(targets[k, :, :].view(-1, 1), outputs[k, :, :].view(-1, 1), n_coefficients)\n",
        "        D[k:k+1, :, :] = Dk\n",
        "\n",
        "    # Calculate shape loss using SoftDTWBatch\n",
        "    loss_shape = softdtw_batch(D, gamma)\n",
        "\n",
        "    # Calculate temporal loss\n",
        "    path_dtw = PathDTWBatch.apply\n",
        "    path = path_dtw(D, gamma)\n",
        "    Omega = pairwise_distances(torch.arange(1, N_output + 1).view(N_output, 1).float()).to(device)\n",
        "    loss_temporal = torch.sum(path * Omega) / (N_output * N_output)\n",
        "\n",
        "    # Combine shape and temporal loss\n",
        "    loss = 0.5 * loss_shape + 0.5 * loss_temporal\n",
        "    return loss, loss_shape, loss_temporal\n"
      ],
      "metadata": {
        "id": "msnD-EbchDKw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6003SNjf3H_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foldt_loss(outputs, targets, alpha, gamma, device, n_coefficients=6):\n",
        "    # outputs, targets: shape (batch_size, N_output, 1)\n",
        "    batch_size, N_output = outputs.shape[0:2]\n",
        "    loss_shape = 0\n",
        "    softdtw_batch = SoftDTWBatch.apply\n",
        "    D = torch.zeros((batch_size, N_output, N_output)).to(device)\n",
        "\n",
        "    for k in range(batch_size):\n",
        "        # Calculate the combined distances (DTW + Fourier)\n",
        "        Dk = pairwise_distances_with_fourier(targets[k, :, :].view(-1, 1), outputs[k, :, :].view(-1, 1), n_coefficients)\n",
        "        D[k:k+1, :, :] = Dk\n",
        "\n",
        "    # Calculate shape loss using SoftDTWBatch\n",
        "    loss_shape = softdtw_batch(D, gamma)\n",
        "\n",
        "    # Calculate temporal loss\n",
        "    path_dtw = PathDTWBatch.apply\n",
        "    path = path_dtw(D, gamma)\n",
        "    Omega = pairwise_distances(torch.arange(1, N_output + 1).view(N_output, 1).float()).to(device)\n",
        "    loss_temporal = torch.sum(path * Omega) / (N_output * N_output)\n",
        "\n",
        "    # Combine shape and temporal loss\n",
        "    loss = 0.9 * loss_shape + 0.1 * loss_temporal\n",
        "    return loss, loss_shape, loss_temporal\n",
        "\n",
        "\n",
        "encoder = EncoderRNN(input_size=1, hidden_size=128, num_grulstm_layers=1, batch_size=batch_size).to(device)\n",
        "decoder = DecoderRNN(input_size=1, hidden_size=128, num_grulstm_layers=1,fc_units=16, output_size=1).to(device)\n",
        "net_gru_mor = Net_GRU(encoder,decoder, N_output, device).to(device)\n",
        "train_model(net_gru_mor,loss_type='foldt',learning_rate=0.001, epochs=500, gamma=gamma, print_every=50, eval_every=50,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FA-NQrD24aS",
        "outputId": "d9604beb-a7b8-4158-dc01-8bc47d357d3e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0  loss  3.301307201385498  loss shape  3.6681177616119385  loss temporal  1.1307285603834316e-05\n",
            " Eval mse=  0.14091452583670616  dtw=  1.125239167978107  tdi=  0.0\n",
            "epoch  50  loss  0.19098106026649475  loss shape  0.1570797562599182  loss temporal  0.49609291553497314\n",
            " Eval mse=  0.019122661091387272  dtw=  0.41189560804597336  tdi=  0.4157000000000002\n",
            "epoch  100  loss  0.16224819421768188  loss shape  0.12959107756614685  loss temporal  0.4561622142791748\n",
            " Eval mse=  0.012047890573740005  dtw=  0.3209455680822627  tdi=  0.6096499999999999\n",
            "epoch  150  loss  0.11904306709766388  loss shape  0.08389230072498322  loss temporal  0.4354000389575958\n",
            " Eval mse=  0.011603337246924639  dtw=  0.31611196137656755  tdi=  0.5563\n",
            "epoch  200  loss  0.11948588490486145  loss shape  0.08528485894203186  loss temporal  0.42729508876800537\n",
            " Eval mse=  0.010952380020171404  dtw=  0.30356173961424304  tdi=  0.6508500000000002\n",
            "epoch  250  loss  0.1286647766828537  loss shape  0.09269434958696365  loss temporal  0.4523985981941223\n",
            " Eval mse=  0.010513358749449253  dtw=  0.29312864218684287  tdi=  0.7602\n",
            "epoch  300  loss  0.11805038154125214  loss shape  0.08134455978870392  loss temporal  0.4484027028083801\n",
            " Eval mse=  0.010623583570122719  dtw=  0.2949526158250221  tdi=  0.7869000000000002\n",
            "epoch  350  loss  0.10970732569694519  loss shape  0.07211662083864212  loss temporal  0.44802361726760864\n",
            " Eval mse=  0.010603274684399366  dtw=  0.29409077146885987  tdi=  0.8510499999999996\n",
            "epoch  400  loss  0.09337705373764038  loss shape  0.05650021508336067  loss temporal  0.4252685606479645\n",
            " Eval mse=  0.010186742059886456  dtw=  0.2858094109495327  tdi=  0.8809500000000001\n",
            "epoch  450  loss  0.11494417488574982  loss shape  0.07484535127878189  loss temporal  0.4758336544036865\n",
            " Eval mse=  0.00992938643321395  dtw=  0.2806014059839996  tdi=  0.75465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderRNN(input_size=1, hidden_size=128, num_grulstm_layers=1, batch_size=batch_size).to(device)\n",
        "decoder = DecoderRNN(input_size=1, hidden_size=128, num_grulstm_layers=1,fc_units=16, output_size=1).to(device)\n",
        "net_gru_d = Net_GRU(encoder,decoder, N_output, device).to(device)\n",
        "train_model(net_gru_d,loss_type='dilate',learning_rate=0.001, epochs=500, gamma=gamma, print_every=50, eval_every=50,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqmHRyYmj7FR",
        "outputId": "1fa09b68-70b5-4d18-b0bc-d16eaf704700"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0  loss  2.5603296756744385  loss shape  5.120659351348877  loss temporal  2.642863683975649e-13\n",
            " Eval mse=  0.4212789535522461  dtw=  2.017669797589545  tdi=  0.0\n",
            "epoch  50  loss  0.22248846292495728  loss shape  0.09670469164848328  loss temporal  0.3482722342014313\n",
            " Eval mse=  0.016590879764407873  dtw=  0.32602285216269244  tdi=  0.9183499999999998\n",
            "epoch  100  loss  0.19872646033763885  loss shape  0.0017260723980143666  loss temporal  0.39572685956954956\n",
            " Eval mse=  0.015206643380224705  dtw=  0.3130692204140725  tdi=  0.6131000000000002\n",
            "epoch  150  loss  0.2000613957643509  loss shape  0.0005584281752817333  loss temporal  0.39956435561180115\n",
            " Eval mse=  0.014739493373781443  dtw=  0.3094957848053683  tdi=  0.6293500000000003\n",
            "epoch  200  loss  0.17989219725131989  loss shape  -0.004184909630566835  loss temporal  0.3639692962169647\n",
            " Eval mse=  0.01305232010781765  dtw=  0.29352626605637117  tdi=  0.7777000000000001\n",
            "epoch  250  loss  0.18893012404441833  loss shape  0.05948875471949577  loss temporal  0.3183715045452118\n",
            " Eval mse=  0.012897544540464878  dtw=  0.2931027651778294  tdi=  0.7912000000000001\n",
            "epoch  300  loss  0.17934276163578033  loss shape  0.008169451728463173  loss temporal  0.35051608085632324\n",
            " Eval mse=  0.01263092178851366  dtw=  0.2907509933465915  tdi=  0.82075\n",
            "epoch  350  loss  0.1767706274986267  loss shape  0.01760551892220974  loss temporal  0.3359357416629791\n",
            " Eval mse=  0.012600538320839405  dtw=  0.2905997186399625  tdi=  0.82225\n",
            "epoch  400  loss  0.16346237063407898  loss shape  0.014329883269965649  loss temporal  0.31259486079216003\n",
            " Eval mse=  0.012101258616894484  dtw=  0.2881186407433446  tdi=  0.8762000000000001\n",
            "epoch  450  loss  0.17703543603420258  loss shape  0.010217471048235893  loss temporal  0.3438534140586853\n",
            " Eval mse=  0.012704543303698301  dtw=  0.29141923081440735  tdi=  0.7765000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== PS LOSS FUNCTIONS ====================\n",
        "def create_patches(x, patch_len, stride):\n",
        "    \"\"\"Create patches from time series data\"\"\"\n",
        "    # x shape: [B, L, C]\n",
        "    B, L, C = x.shape\n",
        "    num_patches = (L - patch_len) // stride + 1\n",
        "    patches = x.unfold(1, patch_len, stride)  # [B, num_patches, C, patch_len]\n",
        "    patches = patches.permute(0, 2, 1, 3)  # [B, C, num_patches, patch_len]\n",
        "    return patches\n",
        "\n",
        "\n",
        "def fourier_based_adaptive_patching(true, pred, patch_len_threshold=20):\n",
        "    \"\"\"Determine patch size based on frequency analysis\"\"\"\n",
        "    # true, pred shape: [B, L, C]\n",
        "    true_fft = torch.fft.rfft(true, dim=1)\n",
        "    frequency_list = torch.abs(true_fft).mean(0).mean(-1)\n",
        "    frequency_list[:1] = 0.0\n",
        "    top_index = torch.argmax(frequency_list)\n",
        "    period = max(true.shape[1] // max(top_index, 1), 4)\n",
        "    patch_len = min(period // 2, patch_len_threshold)\n",
        "    patch_len = max(patch_len, 4)  # Minimum patch length\n",
        "    stride = max(patch_len // 2, 1)\n",
        "\n",
        "    # Patching\n",
        "    true_patch = create_patches(true, patch_len, stride=stride)\n",
        "    pred_patch = create_patches(pred, patch_len, stride=stride)\n",
        "\n",
        "    return true_patch, pred_patch\n",
        "\n",
        "\n",
        "def patch_wise_structural_loss(true_patch, pred_patch):\n",
        "    \"\"\"Calculate structural losses at patch level\"\"\"\n",
        "    # true_patch, pred_patch shape: [B, C, num_patches, patch_len]\n",
        "\n",
        "    # Calculate mean\n",
        "    true_patch_mean = torch.mean(true_patch, dim=-1, keepdim=True)\n",
        "    pred_patch_mean = torch.mean(pred_patch, dim=-1, keepdim=True)\n",
        "\n",
        "    # Calculate variance and standard deviation\n",
        "    true_patch_var = torch.var(true_patch, dim=-1, keepdim=True, unbiased=False)\n",
        "    pred_patch_var = torch.var(pred_patch, dim=-1, keepdim=True, unbiased=False)\n",
        "    true_patch_std = torch.sqrt(true_patch_var + 1e-8)\n",
        "    pred_patch_std = torch.sqrt(pred_patch_var + 1e-8)\n",
        "\n",
        "    # Calculate Covariance\n",
        "    true_pred_patch_cov = torch.mean(\n",
        "        (true_patch - true_patch_mean) * (pred_patch - pred_patch_mean),\n",
        "        dim=-1, keepdim=True\n",
        "    )\n",
        "\n",
        "    # 1. Calculate linear correlation loss\n",
        "    patch_linear_corr = (true_pred_patch_cov + 1e-5) / (true_patch_std * pred_patch_std + 1e-5)\n",
        "    linear_corr_loss = (1.0 - patch_linear_corr).mean()\n",
        "\n",
        "    # 2. Calculate variance (KL divergence)\n",
        "    true_patch_softmax = torch.softmax(true_patch, dim=-1)\n",
        "    pred_patch_softmax = torch.log_softmax(pred_patch, dim=-1)\n",
        "    kl_loss = torch.nn.functional.kl_div(\n",
        "        pred_patch_softmax, true_patch_softmax, reduction='none'\n",
        "    )\n",
        "    var_loss = kl_loss.sum(dim=-1).mean()\n",
        "\n",
        "    # 3. Mean loss\n",
        "    mean_loss = torch.abs(true_patch_mean - pred_patch_mean).mean()\n",
        "\n",
        "    return linear_corr_loss, var_loss, mean_loss\n",
        "\n",
        "\n",
        "def ps_loss(true, pred, patch_len_threshold=20, use_dynamic_weighting=False):\n",
        "    \"\"\"\n",
        "    Patch-wise Structural Loss\n",
        "    Args:\n",
        "        true: ground truth [B, L, C]\n",
        "        pred: predictions [B, L, C]\n",
        "        patch_len_threshold: maximum patch length\n",
        "        use_dynamic_weighting: whether to use gradient-based dynamic weighting\n",
        "    \"\"\"\n",
        "    # Ensure correct shape [B, L, C]\n",
        "    if len(true.shape) == 2:\n",
        "        true = true.unsqueeze(-1)\n",
        "        pred = pred.unsqueeze(-1)\n",
        "\n",
        "    # Fourier based adaptive patching\n",
        "    true_patch, pred_patch = fourier_based_adaptive_patching(\n",
        "        true, pred, patch_len_threshold\n",
        "    )\n",
        "\n",
        "    # Patch-wise structural loss\n",
        "    corr_loss, var_loss, mean_loss = patch_wise_structural_loss(true_patch, pred_patch)\n",
        "\n",
        "    # Simple weighted combination (without gradient-based weighting for compatibility)\n",
        "    if use_dynamic_weighting:\n",
        "        # Calculate similarities for weighting\n",
        "        true_mean = torch.mean(true, dim=1, keepdim=True)\n",
        "        pred_mean = torch.mean(pred, dim=1, keepdim=True)\n",
        "        true_var = torch.var(true, dim=1, keepdim=True, unbiased=False)\n",
        "        pred_var = torch.var(pred, dim=1, keepdim=True, unbiased=False)\n",
        "        true_std = torch.sqrt(true_var + 1e-8)\n",
        "        pred_std = torch.sqrt(pred_var + 1e-8)\n",
        "\n",
        "        true_pred_cov = torch.mean((true - true_mean) * (pred - pred_mean), dim=1, keepdim=True)\n",
        "        linear_sim = (true_pred_cov + 1e-5) / (true_std * pred_std + 1e-5)\n",
        "        linear_sim = (1.0 + linear_sim) * 0.5\n",
        "        var_sim = (2 * true_std * pred_std + 1e-5) / (true_var + pred_var + 1e-5)\n",
        "\n",
        "        # Adaptive weights\n",
        "        alpha = 1.0\n",
        "        beta = 1.0\n",
        "        gamma = torch.mean(linear_sim * var_sim).detach()\n",
        "\n",
        "        total_ps_loss = alpha * corr_loss + beta * var_loss + gamma * mean_loss\n",
        "    else:\n",
        "        # Fixed weights\n",
        "        total_ps_loss = corr_loss + var_loss + mean_loss\n",
        "\n",
        "    return total_ps_loss, corr_loss, var_loss, mean_loss\n",
        "\n",
        "\n",
        "# ==================== TRAINING FUNCTION ====================\n",
        "def train_model(net, loss_type, learning_rate, epochs=1000, gamma=0.001,\n",
        "                print_every=50, eval_every=50, verbose=1, Lambda=1, alpha=0.5,\n",
        "                ps_lambda=1.0, patch_len_threshold=20, use_dynamic_weighting=False):\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, target = data\n",
        "            inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
        "            target = torch.tensor(target, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Ensure target has shape [B, L, C]\n",
        "            if len(target.shape) == 2:\n",
        "                target = target.unsqueeze(-1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Ensure outputs has shape [B, L, C]\n",
        "            if len(outputs.shape) == 2:\n",
        "                outputs = outputs.unsqueeze(-1)\n",
        "\n",
        "            # Initialize losses\n",
        "            loss_mse = torch.tensor(0.0, device=device)\n",
        "            loss_shape = torch.tensor(0.0, device=device)\n",
        "            loss_temporal = torch.tensor(0.0, device=device)\n",
        "            loss_ps_corr = torch.tensor(0.0, device=device)\n",
        "            loss_ps_var = torch.tensor(0.0, device=device)\n",
        "            loss_ps_mean = torch.tensor(0.0, device=device)\n",
        "\n",
        "            # Calculate loss based on loss_type\n",
        "            if loss_type == 'mse':\n",
        "                loss_mse = criterion(target, outputs)\n",
        "                loss = loss_mse\n",
        "\n",
        "            elif loss_type == 'dilate':\n",
        "                loss, loss_shape, loss_temporal = dilate_loss(\n",
        "                    target, outputs, alpha, gamma, device\n",
        "                )\n",
        "\n",
        "            elif loss_type == 'ps':\n",
        "                loss_ps_total, loss_ps_corr, loss_ps_var, loss_ps_mean = ps_loss(\n",
        "                    target, outputs, patch_len_threshold, use_dynamic_weighting\n",
        "                )\n",
        "                loss = loss_ps_total\n",
        "\n",
        "            elif loss_type == 'mse+ps':\n",
        "                # Combined MSE + PS Loss\n",
        "                loss_mse = criterion(target, outputs)\n",
        "                loss_ps_total, loss_ps_corr, loss_ps_var, loss_ps_mean = ps_loss(\n",
        "                    target, outputs, patch_len_threshold, use_dynamic_weighting\n",
        "                )\n",
        "                loss = loss_mse + ps_lambda * loss_ps_total\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown loss_type: {loss_type}\")\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if verbose and (epoch % print_every == 0):\n",
        "            if loss_type in ['dilate', 'dilate+ps']:\n",
        "                print(f'Epoch {epoch:4d} | Loss: {loss.item():.6f} | '\n",
        "                      f'Shape: {loss_shape.item():.6f} | Temporal: {loss_temporal.item():.6f}')\n",
        "            elif loss_type in ['ps', 'mse+ps', 'dilate+ps']:\n",
        "                print(f'Epoch {epoch:4d} | Loss: {loss.item():.6f} | ')\n",
        "            else:\n",
        "                print(f'Epoch {epoch:4d} | MSE Loss: {loss_mse.item():.6f}')\n",
        "\n",
        "            if epoch % eval_every == 0:\n",
        "                eval_model(net, testloader, gamma, verbose=1)\n",
        "\n",
        "\n",
        "def eval_model(net, loader, gamma, verbose=1):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    losses_mse = []\n",
        "    losses_dtw = []\n",
        "    losses_tdi = []\n",
        "\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(loader, 0):\n",
        "            inputs, target = data\n",
        "            inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
        "            target = torch.tensor(target, dtype=torch.float32).to(device)\n",
        "\n",
        "            if len(target.shape) == 2:\n",
        "                target = target.unsqueeze(-1)\n",
        "\n",
        "            batch_size, N_output = target.shape[0:2]\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            if len(outputs.shape) == 2:\n",
        "                outputs = outputs.unsqueeze(-1)\n",
        "\n",
        "            # MSE\n",
        "            loss_mse = criterion(target, outputs)\n",
        "\n",
        "            # DTW and TDI\n",
        "            loss_dtw, loss_tdi = 0, 0\n",
        "            for k in range(batch_size):\n",
        "                target_k_cpu = target[k, :, 0].detach().cpu().numpy()\n",
        "                output_k_cpu = outputs[k, :, 0].detach().cpu().numpy()\n",
        "\n",
        "                path, sim = dtw_path(target_k_cpu, output_k_cpu)\n",
        "                loss_dtw += sim\n",
        "\n",
        "                Dist = 0\n",
        "                for i, j in path:\n",
        "                    Dist += (i - j) * (i - j)\n",
        "                loss_tdi += Dist / (N_output * N_output)\n",
        "\n",
        "            loss_dtw = loss_dtw / batch_size\n",
        "            loss_tdi = loss_tdi / batch_size\n",
        "\n",
        "            losses_mse.append(loss_mse.item())\n",
        "            losses_dtw.append(loss_dtw)\n",
        "            losses_tdi.append(loss_tdi)\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    if verbose:\n",
        "        print(f'  Eval | MSE: {np.mean(losses_mse):.6f} | '\n",
        "              f'DTW: {np.mean(losses_dtw):.6f} | TDI: {np.mean(losses_tdi):.6f}')\n",
        "\n",
        "\n",
        "# Configuration du modèle comme avant\n",
        "encoder = EncoderRNN(input_size=1, hidden_size=128, num_grulstm_layers=1, batch_size=batch_size).to(device)\n",
        "decoder = DecoderRNN(input_size=1, hidden_size=128, num_grulstm_layers=1, fc_units=16, output_size=1).to(device)\n",
        "net_gru_ps = Net_GRU(encoder, decoder, N_output, device).to(device)\n",
        "\n",
        "train_model(net_gru_ps, loss_type='mse+ps', learning_rate=0.001,\n",
        "            epochs=500, ps_lambda=1.0, patch_len_threshold=20)\n"
      ],
      "metadata": {
        "id": "KwrQ-yYBKYg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef555039-6322-417e-ed77-6e6ff9645b72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0 | Loss: 1.004614 | \n",
            "  Eval | MSE: 0.064562 | DTW: 0.709390 | TDI: 0.049800\n",
            "Epoch   50 | Loss: 0.459615 | \n",
            "  Eval | MSE: 0.023211 | DTW: 0.384241 | TDI: 0.363600\n",
            "Epoch  100 | Loss: 0.370922 | \n",
            "  Eval | MSE: 0.018777 | DTW: 0.393808 | TDI: 0.100900\n",
            "Epoch  150 | Loss: 0.376059 | \n",
            "  Eval | MSE: 0.019721 | DTW: 0.416971 | TDI: 0.092100\n",
            "Epoch  200 | Loss: 0.363192 | \n",
            "  Eval | MSE: 0.020267 | DTW: 0.425185 | TDI: 0.103500\n",
            "Epoch  250 | Loss: 0.371626 | \n",
            "  Eval | MSE: 0.020566 | DTW: 0.429255 | TDI: 0.092350\n",
            "Epoch  300 | Loss: 0.367440 | \n",
            "  Eval | MSE: 0.020760 | DTW: 0.431813 | TDI: 0.100100\n",
            "Epoch  350 | Loss: 0.354366 | \n",
            "  Eval | MSE: 0.020693 | DTW: 0.430940 | TDI: 0.113050\n",
            "Epoch  400 | Loss: 0.376264 | \n",
            "  Eval | MSE: 0.020688 | DTW: 0.430864 | TDI: 0.119650\n",
            "Epoch  450 | Loss: 0.356417 | \n",
            "  Eval | MSE: 0.020779 | DTW: 0.432055 | TDI: 0.090700\n"
          ]
        }
      ]
    }
  ]
}